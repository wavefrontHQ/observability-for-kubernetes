---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gauge.kubernetes.process-stats.total-disk-write-throughput-bytes
  namespace: observability-system
  labels:
    purpose: cron-script
data:
  script.pxl: |
    import px

    ns_per_ms = 1000 * 1000
    ns_per_s = 1000 * ns_per_ms

    # Window size to use on time_ column for bucketing.
    window_ns = px.DurationNanos(10 * ns_per_s)

    df = px.DataFrame(table='process_stats', start_time=px.plugin.start_time, end_time=px.plugin.end_time)

    df.pod = df.ctx['pod_name']

    # Add context
    df.node = df.ctx['node']
    df.container = df.ctx['container_name']
    df.pod_id = df.ctx['pod_id']
    df.namespace = px.pod_name_to_namespace(df.pod)

    # The Pod Name, as assigned by Pixie, includes the Pod's namespace in the name. ex. namespace/pod-name.
    # This replace ensures that we only have the pod name
    df.pod_name = px.replace('.*\/', df.ctx['pod'], '')

    df.service = df.ctx['service']
    df.deployment = df.ctx['deployment']
    df.cluster_name = px.vizier_name()
    df.aria_provider = 'Kubernetes'
    df.aria_service = 'Workload'
    df.timestamp = px.bin(df.time_, window_ns)

    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby([
      'aria_provider',
      'aria_service',
      'cluster_name',
      'container',
      'deployment',
      'namespace',
      'node',
      'pod_name',
      'pod',
      'service',
      'timestamp',
      'upid',
    ]).agg(
      wchar_bytes_max=('wchar_bytes', px.max),
      wchar_bytes_min=('wchar_bytes', px.min),
    )

    # Next calculate cpu usage and memory stats per window.
    df.total_disk_write_throughput = (df.wchar_bytes_max - df.wchar_bytes_min) / window_ns

    # Then aggregate process individual process metrics.
    df = df.groupby([
        'aria_provider',
        'aria_service',
        'cluster_name',
        'container',
        'deployment',
        'namespace',
        'node',
        'pod_name',
        'pod',
        'service',
        'timestamp',
    ]).agg(
      total_disk_write_throughput=('total_disk_write_throughput', px.sum),
    )

    df['time_'] = df['timestamp']

    px.export(
      df, px.otel.Data(
        resource={
          # User must specify service_name
          'service.name': "vmware",
          'aria.provider': df.aria_provider,
          'aria.service': df.aria_service,
          'kubernetes.cluster.name': df.cluster_name,
          'kubernetes.container.name': df.container,
          'kubernetes.deployment.name': df.deployment,
          'kubernetes.namespace.name': df.namespace,
          'kubernetes.pod.fullname': df.pod,
          'kubernetes.pod.name': df.pod_name,
          'kubernetes.service.name': df.service,
          'service.instance.id': df.pod,
        },
        data=[
          px.otel.metric.Gauge(
            name='kubernetes.process_stats.total_disk_write_throughput_bytes',
            description='Total disk write throughput in bytes',
            value=df.total_disk_write_throughput,
            attributes={},
          ),
        ],
      ),
    )
  configs.yaml: |
    otelEndpointConfig:
      insecure: true
      url: aria-telegraf-collector.observability-system.svc.cluster.local:4317
  cron.yaml: |
    frequency_s: 67